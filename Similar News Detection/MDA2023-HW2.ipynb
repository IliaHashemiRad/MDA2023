{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262a1420",
   "metadata": {},
   "source": [
    "# Data Mining Course Spark Exercise\n",
    "## Sharif University of Technology\n",
    "\n",
    "In this notebook we are going to analyze farsi news. \n",
    "\n",
    "## Warning: RDD api only\n",
    "You **can not** use Dataframe, Dataset, mllib, ml, ... apis of spark in this exercise. You should only use the [RDD api](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75576da1",
   "metadata": {},
   "source": [
    "# Please enter your name below:\n",
    "# Name: Ilia Hashemi Rad\n",
    "# Student Number: 99102456"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d038ab",
   "metadata": {},
   "source": [
    "# Section 1: Dataset prepartition\n",
    "This section you need to download [dataset](https://drive.google.com/file/d/1bRxHQDzPr6wDimbM7b89H47kH8O3YV8Y/view?usp=sharing) in a directory you work. After that run the below cell to untar the datase.\n",
    "\n",
    "**Note 1: Don't change the below command.**\n",
    "\n",
    "**Note 2: If you use Windows OS, unzip the dataset manually.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0ae4f",
   "metadata": {},
   "source": [
    "## Install Pypark & Initialization\n",
    "Uncomment this section if you use google colab or local pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237c3551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:05:45.526929200Z",
     "start_time": "2023-12-12T16:05:42.978876400Z"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3706a8d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:12:02.175712800Z",
     "start_time": "2023-12-12T16:12:02.151692900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/06 21:06:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HW1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13157b6b",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2f4ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T16:12:05.161759700Z",
     "start_time": "2023-12-12T16:12:04.743641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"body\": \"نشست بزرگداشت روز جهانی حافظ شیرازی 20 مهرماه در دانشگاه پکن برگزار می شود.\\\\n\\\\n- ایرنا نوشت: با همکاری مشترک گروه زبان و ادبیات فارسی دانشگاه پکن و رایزنی فرهنگی جمهوری اسلامی ایران در چین، نشست بزرگداشت روز جهانی حافظ شیرازی روز پنجشنبه 20 مهرماه ساعت 10:30 تا 12 در دانشگاه پکن برگزار می شود.\\\\n\\\\nدر این نشست، محسن بختیار، سفیر و نعمت الله ایران زاده، رایزن فرهنگی جمهوری اسلامی ایران در چین، پروفسور چن مینْگ، رییس محترم دانشکده زبان های خارجی دانشگاه پکن، بهادر باقری، استاد مدعو زبان و ادبیات فارسی دانشکده زبان های خارجی دانشگاه پکن، سخنرانی می کنند.\\\\n\\\\nخانم وانگ یی دان، استاد رشته زبان و ادبیات فارسی دانشگاه پکن، شی گوانگ، استاد رشته زبان و ادبیات فارسی و مدیر مرکز پژوهش های فرهنگ ایران در دانشگاه پکن و لیو یینگ جون، استاد و مدیر گروه زبان و ادبیات فارسی دانشگاه پکن از دیگر سخنرانان این نشست هستند.\", \"image_title_url\": \"https://ettelaat.com/files/fa/news/1402/7/20/82679_282.jpg\", \"language\": \"fa\", \"source\": \"موتور جستجوی قطره\", \"title\": \"ادای احترام چینی ها به حافظ +عکس | نشست بزرگداشت روز جهانی\", \"date_published\": 1697070054, \"uid\": \"7de950ad0cf5f22ed1d740feb\", \"url\": \"https://www.ghatreh.com/news/nn14020701825494745088/ادای-احترام-چینی-حافظ-عکس\", \"crawler_timestamp\": 1697070106, \"ingestor_timestamp\": 1697086977, \"summary\": \"نشست بزرگداشت روز جهانی حافظ شیرازی 20 مهرماه در دانشگاه پکن برگزار می شود. - ایرنا نوشت:\\xa0با همکاری مشترک گروه ز�\", \"hostname\": \"ghatreh.com\", \"parser_categories\": [], \"keywords\": [\"رییس محترم دانشکده زبان\\u200cهای خارجی دانشگاه پکن\", \"استاد رشته زبان و ادبیات فارسی\", \"نشست بزرگداشت روز جهانی حافظ شیرازی\", \"رایزن فرهنگی جمهوری اسلامی ایران\"], \"parser_keyword\": [\"ادبیات فارسی\", \"دانشگاه\", \"جمهوری اسلامی ایران\", \"ادبیات\", \"زبان\", \"فارسی\", \"جمهوری اسلامی\"], \"author\": \"حسن قلي پور خطير\", \"processed_body\": \"نشست بزرگداشت روز جهانی حافظ شیرازی 20 مهرماه در دانشگاه پکن برگزار می شود.\\\\n\\\\n- ایرنا نوشت: با همکاری مشترک گروه زبان و ادبیات فارسی دانشگاه پکن و رایزنی فرهنگی جمهوری اسلامی ایران در چین، نشست بزرگداشت روز جهانی حافظ شیرازی روز پنجشنبه 20 مهرماه ساعت 10:30 تا 12 در دانشگاه پکن برگزار می شود.\\\\n\\\\nدر این نشست، محسن بختیار، سفیر و نعمت الله ایران زاده، رایزن فرهنگی جمهوری اسلامی ایران در چین، پروفسور چن مینْگ، رییس محترم دانشکده زبان های خارجی دانشگاه پکن، بهادر باقری، استاد مدعو زبان و ادبیات فارسی دانشکده زبان های خارجی دانشگاه پکن، سخنرانی می کنند.\\\\n\\\\nخانم وانگ یی دان، استاد رشته زبان و ادبیات فارسی دانشگاه پکن، شی گوانگ، استاد رشته زبان و ادبیات فارسی و مدیر مرکز پژوهش های فرهنگ ایران در دانشگاه پکن و لیو یینگ جون، استاد و مدیر گروه زبان و ادبیات فارسی دانشگاه پکن از دیگر سخنرانان این نشست هستند.\", \"categories\": [\"politics\"], \"ner_tags\": [{\"type\": \"NORP\", \"entity\": \"چینی\"}, {\"type\": \"EVE\", \"entity\": \"نشست بزرگداشت روز جهانی حافظ شیرازی\"}, {\"type\": \"FAC\", \"entity\": \"دانشگاه پکن\"}, {\"type\": \"ORG\", \"entity\": \"ایرنا\"}, {\"type\": \"EVE\", \"entity\": \"نشست بزرگداشت روز جهانی حافظ شیرازی\"}, {\"type\": \"FAC\", \"entity\": \"دانشگاه پکن\"}, {\"type\": \"ORG\", \"entity\": \"ایرنا\"}, {\"type\": \"ORG\", \"entity\": \"گروه زبان و ادبیات فارسی دانشگاه پکن و رایزنی فرهنگی جمهوری اسلامی ایران\"}, {\"type\": \"GPE\", \"entity\": \"چین\"}, {\"type\": \"EVE\", \"entity\": \"نشست بزرگداشت روز جهانی حافظ شیرازی\"}, {\"type\": \"FAC\", \"entity\": \"دانشگاه پکن\"}, {\"type\": \"PER\", \"entity\": \"محسن بختیار\"}, {\"type\": \"POST\", \"entity\": \"سفیر\"}, {\"type\": \"PER\", \"entity\": \"نعمت الله ایران زاده\"}, {\"type\": \"POST\", \"entity\": \"رایزن فرهنگی جمهوری اسلامی ایران\"}, {\"type\": \"GPE\", \"entity\": \"چین\"}, {\"type\": \"PER\", \"entity\": \"پروفسور چن مینگ\"}, {\"type\": \"POST\", \"entity\": \"رییس محترم دانشکده زبان\\u200cهای خارجی دانشگاه پکن\"}, {\"type\": \"PER\", \"entity\": \"بهادر باقری\"}, {\"type\": \"POST\", \"entity\": \"استاد مدعو زبان و ادبیات فارسی دانشکده زبان\\u200cهای خارجی دانشگاه پکن\"}, {\"type\": \"PER\", \"entity\": \"وانگ یی دان\"}, {\"type\": \"POST\", \"entity\": \"استاد رشته زبان و ادبیات فارسی دانشگاه پکن\"}, {\"type\": \"PER\", \"entity\": \"شی گوانگ\"}, {\"type\": \"POST\", \"entity\": \"استاد رشته زبان و ادبیات فارسی\"}, {\"type\": \"POST\", \"entity\": \"مدیر مرکز پژوهش\\u200cهای فرهنگ ایران\"}, {\"type\": \"FAC\", \"entity\": \"دانشگاه پکن\"}, {\"type\": \"PER\", \"entity\": \"لیو یینگ جون\"}, {\"type\": \"POST\", \"entity\": \"استاد\"}, {\"type\": \"POST\", \"entity\": \"مدیر گروه زبان و ادبیات فارسی دانشگاه پکن\"}]}']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_rdd = sc.textFile(\"news_data.jsonl\")\n",
    "\n",
    "news_rdd.takeSample(False, 1, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa39393",
   "metadata": {},
   "source": [
    "## Shingling : 2 words without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1922c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('68feae4bbbedc2d54adbb2369', [('سرمربی', 'ملوان'), ('ملوان', 'تصاویر'), ('تصاویر', 'وایرالی'), ('وایرالی', 'دیدار'), ('دیدار', 'نساجی'), ('نساجی', 'پست'), ('پست', 'جالب'), ('جالب', 'منتشر'), ('منتشر', 'گزارش'), ('گزارش', 'ورزش'), ('ورزش', 'سه'), ('سه', 'مهدی'), ('مهدی', 'تارتار'), ('تارتار', 'خوشحالی'), ('خوشحالی', 'عجیب'), ('عجیب', 'غریب'), ('غریب', 'دقیقه'), ('دقیقه', 'چهرههای'), ('چهرههای', 'جالب'), ('جالب', 'هفته'), ('هفته', 'لیگ'), ('لیگ', 'برتر'), ('برتر', 'اختصاص'), ('اختصاص', 'موردتوجه'), ('موردتوجه', 'هواداران'), ('هواداران', 'فوتبال'), ('فوتبال', 'قرار'), ('قرار', 'تارتار'), ('تارتار', 'انتشار'), ('انتشار', 'پست'), ('پست', 'اینستاگرامی'), ('اینستاگرامی', 'تصاویر'), ('تصاویر', 'جشن'), ('جشن', 'خوشحالی'), ('خوشحالی', 'نوشته'), ('نوشته', 'جادوی'), ('جادوی', 'طرفداران'), ('طرفداران', 'انزلی'), ('انزلی', 'سرمربی'), ('سرمربی', 'ملوان'), ('ملوان', 'وعده'), ('وعده', 'تیمش'), ('تیمش', 'ادامه'), ('ادامه', 'فصل'), ('فصل', 'جنگید'), ('جنگید', 'رضایت'), ('رضایت', 'هواداران'), ('هواداران', 'جلب'), ('جلب', 'نوشته'), ('نوشته', 'ادامه'), ('ادامه', 'عکسها'), ('عکسها', 'متحیر'), ('متحیر', 'میشوم'), ('میشوم', 'یادم'), ('یادم', 'نمیآید'), ('نمیآید', 'شوری'), ('شوری', 'جادوی'), ('جادوی', 'سکو'), ('سکو', 'حس'), ('حس', 'انرژی'), ('انرژی', 'میدهید'), ('میدهید', 'لحظه'), ('لحظه', 'پیروزی'), ('پیروزی', 'وصفناشدنی'), ('وصفناشدنی', 'سکوها'), ('سکوها', 'خط'), ('خط', 'پای'), ('پای', 'تلویزیونها'), ('تلویزیونها', 'مستطیل'), ('مستطیل', 'سبز'), ('سبز', 'همدل'), ('همدل', 'یکصدا'), ('یکصدا', 'میرویم'), ('میرویم', 'باهم'), ('باهم', 'میجنگیم'), ('میجنگیم', 'باهم'), ('باهم', 'میخندیم'), ('میخندیم', 'هرروز'), ('هرروز', 'تلاش'), ('تلاش', 'خنده'), ('خنده', 'نگه'), ('نگه', 'پیروزی'), ('پیروزی', 'مردان'), ('مردان', 'نیک'), ('نیک', 'چمن'), ('چمن', 'مردمان'), ('مردمان', 'نیک'), ('نیک', 'سکو')])]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Parse the JSON lines and extract the UID and body text from each line\n",
    "parsed_news_rdd = news_rdd.map(lambda line: (json.loads(line)['uid'], json.loads(line)['body']))\n",
    "\n",
    "# Define a function to remove useless characters including '\\n', '\\u200c', and '\\\\n'\n",
    "def remove_useless_characters(text):\n",
    "    # Remove special characters, digits, and unwanted unicode characters\n",
    "    text = re.sub(r'\\\\|\\\\n|\\\\u200c|\\n|پایان پیام|[^آ-ی\\s]', ' ', text)\n",
    "\n",
    "    # Define a regular expression pattern that matches one or more spaces\n",
    "    pattern = re.compile(r\" +\")\n",
    "    # Apply the pattern to the text and replace the matches with a single space\n",
    "    text = pattern.sub(\" \", text)\n",
    "    return text\n",
    "\n",
    "# Define a function to determine a list of Farsi stop words\n",
    "def load_stop_words(file_paths):\n",
    "    stop_words_set = set()\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            stop_words_set.update(file.read().splitlines())\n",
    "    return stop_words_set\n",
    "\n",
    "# Define a function to create shinglings of words.\n",
    "def generate_ngrams(text, n):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Generate n-grams using a sliding window of size n\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# List of paths to your stop words files\n",
    "stop_words_files = ['verbal.txt', 'persian.txt', 'short.txt', 'chars.txt', 'nonverbal.txt']\n",
    "\n",
    "# Load stop words from files\n",
    "stop_words = load_stop_words(stop_words_files)\n",
    "\n",
    "# Remove useless characters from the body text in parsed_news_rdd\n",
    "clean_news_rdd = parsed_news_rdd.map(lambda x: (x[0], remove_useless_characters(x[1])))\n",
    "\n",
    "# Remove the stop words from clean_news_rdd without splitting the text\n",
    "clean_news_rdd = clean_news_rdd.map(lambda x: (x[0], \" \".join(filter(lambda w: w not in stop_words, x[1].split()))))\n",
    "\n",
    "# Process the cleaned news RDD by generating bi-grams (2-grams)\n",
    "processed_news_rdd = clean_news_rdd.map(lambda x: (x[0], generate_ngrams(x[1], 2)))\n",
    "\n",
    "# Display the processed news RDD for one record\n",
    "print(processed_news_rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269afa9",
   "metadata": {},
   "source": [
    "## Min-Hashing and Signature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c531f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "# Define the number of bands and rows for the Min-Hash algorithm\n",
    "NUM_BAND = 40  # Number of bands\n",
    "NUM_ROW = 3    # Number of rows per band\n",
    "\n",
    "# Calculate the total number of hash functions\n",
    "k = NUM_BAND * NUM_ROW\n",
    "\n",
    "# Define a large prime number for the hash functions\n",
    "p = 4294967311\n",
    "\n",
    "# Generate k random coefficients for the hash functions\n",
    "a = random.sample(range(1, p), k)\n",
    "b = random.sample(range(0, p), k)\n",
    "\n",
    "# Define a function to hash a shingle using the ith hash function\n",
    "def hash_shingle(shingle, i):\n",
    "    shingle = \" \".join(shingle)\n",
    "    return (a[i] * hash(shingle) + b[i]) % p\n",
    "\n",
    "# Define a function to compute the min-hash signature of a set of shingles\n",
    "def min_hash(shingles):\n",
    "    # Initialize a signature vector of length k with positive infinity values\n",
    "    signature = [sys.maxsize] * k\n",
    "    # Loop over each shingle in the set\n",
    "    for shingle in shingles:\n",
    "        # Loop over each hash function\n",
    "        for i in range(k):\n",
    "            # Hash the shingle using the ith hash function\n",
    "            hash_value = hash_shingle(shingle, i)\n",
    "            # Update the ith value of the signature if the hash value is smaller to find the minimum signature\n",
    "            if signature[i] > hash_value:\n",
    "                signature[i] = hash_value\n",
    "    # Return the signature vector\n",
    "    return signature\n",
    "\n",
    "# Min-Hash and Signature Matrix\n",
    "# Apply the min_hash function to each set of shingles in processed_news_rdd\n",
    "# to generate the min-hash signature matrix for each news article\n",
    "signature_matrix = processed_news_rdd.map(lambda x: (x[0], min_hash(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c7a90",
   "metadata": {},
   "source": [
    "## LSH and Candidate Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f415a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:====================================================> (183 + 6) / 189]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('00001f08ded0a4ad2f7618cd6', '2da46f469fd627b1b4f5c927e'), ('000046429e09f6b639b6d8532', '0db68ee6e6f6695e0ecce36be'), ('000046429e09f6b639b6d8532', '115b9fbe166a1641bd72df893'), ('000046429e09f6b639b6d8532', '116e1951e6ec261e106a2ce0f'), ('000046429e09f6b639b6d8532', '12f245b2790078eea668d2179'), ('000046429e09f6b639b6d8532', '1f318e048098dc77b367fc633'), ('000046429e09f6b639b6d8532', '2261fa398dbe5e07ba76ef9e2'), ('000046429e09f6b639b6d8532', '38f030b89882842729e3b165a'), ('000046429e09f6b639b6d8532', '3b92e8f292f26b2a9232b6cb2'), ('000046429e09f6b639b6d8532', '5d3c94171ec1f070ea33a4866')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define a function to hash a signature vector using the ith band\n",
    "def hash_signature(signature, i):\n",
    "    # Extract the band corresponding to the ith band index\n",
    "    band = signature[i * NUM_ROW: (i + 1) * NUM_ROW]\n",
    "    # Calculate the hash value of the band\n",
    "    value = hash(tuple(band))\n",
    "    return (a[i] * value + b[i]) % p\n",
    "\n",
    "# Define a function to generate candidate pairs from a band\n",
    "def generate_candidates(band):\n",
    "    # Group the tuples by their second element using the groupby function\n",
    "    grouped = itertools.groupby(sorted(band, key=lambda x: x[1]), key=lambda x: x[1])\n",
    "    # Initialize an empty list of candidate pairs\n",
    "    candidates = []\n",
    "    # Loop over each group\n",
    "    for _, group in grouped:\n",
    "        # Extract the list of articles in the group\n",
    "        articles = [x[0] for x in group]\n",
    "        # If the group has more than one article, generate all possible pairs\n",
    "        if len(articles) > 1:\n",
    "            for i in range(len(articles)):\n",
    "                for j in range(i + 1, len(articles)):\n",
    "                    # Append the pair to the list of candidates\n",
    "                    candidates.append((articles[i], articles[j]))\n",
    "    # Return the list of candidate pairs\n",
    "    return candidates\n",
    "\n",
    "# LSH and generating candidate pairs\n",
    "\n",
    "# Apply the hash_signature function to each signature vector in the RDD\n",
    "hashed_matrix = signature_matrix.flatMap(lambda x: [(i, (x[0], hash_signature(x[1], i))) for i in range(NUM_BAND)])\n",
    "\n",
    "# Group the hashed matrix by the band index\n",
    "grouped_matrix = hashed_matrix.groupByKey().mapValues(list)\n",
    "\n",
    "# Generate candidate pairs from each band\n",
    "candidate_pairs = grouped_matrix.flatMap(lambda x: generate_candidates(x[1]))\n",
    "\n",
    "# Remove duplicate pairs and sort them by the article ids\n",
    "candidate_pairs = candidate_pairs.distinct().sortBy(lambda x: (x[0], x[1]))\n",
    "\n",
    "# Display the first 10 candidate pairs\n",
    "print(candidate_pairs.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368626a3",
   "metadata": {},
   "source": [
    "## Clustering and Finding Similar News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aca522",
   "metadata": {},
   "source": [
    "#### Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query UID\n",
    "query_uid = 'b272bcaeb94af1c6e9c6ed0aa'\n",
    "\n",
    "# Filter candidate pairs for the given query UID\n",
    "query_candidates = candidate_pairs.filter(lambda x: x[0] == query_uid or x[1] == query_uid).map(lambda x: x[1] if x[0] == query_uid else x[0]).collect()\n",
    "\n",
    "# Retrieve the set of shingles for the query UID\n",
    "query_shingles = set(processed_news_rdd.lookup(query_uid)[0])\n",
    "\n",
    "# Retrieve shingles and UIDs for the candidate pairs\n",
    "candidates_uid_shingles = processed_news_rdd.filter(lambda x: x[0] in query_candidates).collect()\n",
    "\n",
    "# Define the Jaccard similarity threshold\n",
    "jaccard_threshold = 0.3\n",
    "\n",
    "# Define a function to compute the Jaccard similarity\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "    return intersection_size / union_size if union_size > 0 else 0\n",
    "\n",
    "# Find similar news based on Jaccard similarity\n",
    "similar_news_uid = []\n",
    "similar_news = []\n",
    "for candidate_uid, candidate_shingles in candidates_uid_shingles:\n",
    "    candidate_shingles = set(candidate_shingles)\n",
    "    similarity = compute_jaccard_similarity(query_shingles, candidate_shingles)\n",
    "    # Check if the Jaccard similarity is above the threshold\n",
    "    if similarity > jaccard_threshold:\n",
    "        similar_news.append((candidate_uid, similarity))\n",
    "        similar_news_uid.append(candidate_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af94325",
   "metadata": {},
   "source": [
    "#### Validation Of Results for sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10abe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=================================================>   (176 + 13) / 189]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given article title: \n",
      "عرضه 6 خودروی وارداتی در سامانه یکپارچه آغاز شد- اخبار صنعت و تجارت - اخبار اقتصادی تسنیم | Tasnim\n",
      "\n",
      "\n",
      "\n",
      "For the given article, 12 similar news founded:\n",
      "\n",
      "Title: سامانه یکپارچه فروش برای خودروهای وارداتی باز شد\n",
      "\n",
      "UID: 46435999e803005bd47cf74e0, Similarity: 0.9166666666666666\n",
      "\n",
      "\n",
      "Title: جزئیات عرضه ۶ خودروی وارداتی در سامانه یکپارچه | زمان ثبت نام و اسامی خودروها\n",
      "\n",
      "UID: 74ac443c728e00bce09493ba1, Similarity: 0.9032258064516129\n",
      "\n",
      "\n",
      "Title: خبر مهم از سامانه یکپارچه خودرو/متقاضیان خرید خودرو بخوانند\n",
      "\n",
      "UID: e01e651fa146e1ee350faad5b, Similarity: 0.5492957746478874\n",
      "\n",
      "\n",
      "Title:  عرضه ۶ خودروی جدید در سامانه یکپارچه\n",
      "\n",
      "UID: a3be28df0263addf005bae7b9, Similarity: 0.7903225806451613\n",
      "\n",
      "\n",
      "Title: عرضه 6 خودروی وارداتی در سامانه یکپارچه آغاز شد- اخبار صنعت و تجارت - اخبار اقتصادی تسنیم | Tasnim\n",
      "\n",
      "UID: b272bcaeb94af1c6e9c6ed0aa, Similarity: 1.0\n",
      "\n",
      "\n",
      "Title: تویوتا کرولا به لیست خودروهای وارداتی ... - ارانیکو\n",
      "\n",
      "UID: 4e59c26a11124d15c2232ca4f, Similarity: 0.3055555555555556\n",
      "\n",
      "\n",
      "Title:  سامانه یکپارچه فروش با این خودروها باز شد \n",
      "\n",
      "UID: 81a1b7e0102533e8e7fae7595, Similarity: 0.9193548387096774\n",
      "\n",
      "\n",
      "Title: مشخصات و جزئیات 6 خودرو وارداتی عرضه شده از امروز + عکس- اخبار صنعت و تجارت - اخبار اقتصادی تسنیم | Tasnim\n",
      "\n",
      "UID: 061998c8d1a57035f872bd5b7, Similarity: 0.5540540540540541\n",
      "\n",
      "\n",
      "Title: جزئیات عرضه ۶ خودرو وارداتی در سامانه یکپارچه\n",
      "\n",
      "UID: 540201a664ea7d7443ffc4dfb, Similarity: 0.875\n",
      "\n",
      "\n",
      "Title: خبر مهم از سامانه یکپارچه خودرو/متقاضیان خرید خودرو بخوانند - ساناپرس\n",
      "\n",
      "UID: 6c4409fc508f6ab5949c78863, Similarity: 0.5\n",
      "\n",
      "\n",
      "Title: عرضه 6 خودروی وارداتی آغاز شد- اخبار صنعت و تجارت - اخبار اقتصادی تسنیم | Tasnim\n",
      "\n",
      "UID: 1e847d4af5e9ef96104a0498d, Similarity: 0.6176470588235294\n",
      "\n",
      "\n",
      "Title: عرضه 6 خودروی وارداتی آغاز شد - خبرجو۲۴\n",
      "\n",
      "UID: befd21b4c684439d8646860ec, Similarity: 0.5822784810126582\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract the title of the query article\n",
    "query_title = list(set(news_rdd.filter(lambda x: json.loads(x)['uid'] in [query_uid]).map(lambda x: json.loads(x)['title']).collect()))\n",
    "\n",
    "# Extract the title and UID of similar news articles\n",
    "similar_news_list = list(set(news_rdd.filter(lambda x: json.loads(x)['uid'] in similar_news_uid).map(lambda x: (json.loads(x)['uid'], json.loads(x)['title'])).collect()))\n",
    "\n",
    "# Create dictionaries for efficient lookup\n",
    "dict1 = dict(similar_news)\n",
    "dict2 = dict(similar_news_list)\n",
    "\n",
    "# Join the dictionaries to get a list of tuples with common keys(uid)... final result is a list of tuples with uis, similarity and title of each similar news\n",
    "joined_list = [(key, dict1[key], dict2[key]) for key in set(dict1) & set(dict2)]\n",
    "\n",
    "# Display the query article title\n",
    "print('The given article title: ')\n",
    "print(query_title[0])\n",
    "print('\\n\\n')\n",
    "\n",
    "# Display information about similar news articles\n",
    "print(f'For the given article, {len(joined_list)} unique similar news founded:\\n')\n",
    "for uid, similarity, title in joined_list:\n",
    "    print(f\"Title: {title}\\n\")\n",
    "    print(f\"UID: {uid}, Similarity: {similarity}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2c35a",
   "metadata": {},
   "source": [
    "#### sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query UID\n",
    "query_uid = '000046429e09f6b639b6d8532'\n",
    "\n",
    "# Filter candidate pairs for the given query UID\n",
    "query_candidates = candidate_pairs.filter(lambda x: x[0] == query_uid or x[1] == query_uid).map(lambda x: x[1] if x[0] == query_uid else x[0]).collect()\n",
    "\n",
    "# Retrieve the set of shingles for the query UID\n",
    "query_shingles = set(processed_news_rdd.lookup(query_uid)[0])\n",
    "\n",
    "# Retrieve shingles and UIDs for the candidate pairs\n",
    "candidates_uid_shingles = processed_news_rdd.filter(lambda x: x[0] in query_candidates).collect()\n",
    "\n",
    "# Define the Jaccard similarity threshold\n",
    "jaccard_threshold = 0.3\n",
    "\n",
    "# Define a function to compute the Jaccard similarity\n",
    "def compute_jaccard_similarity(set1, set2):\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "    return intersection_size / union_size if union_size > 0 else 0\n",
    "\n",
    "# Find similar news based on Jaccard similarity\n",
    "similar_news_uid = []\n",
    "similar_news = []\n",
    "for candidate_uid, candidate_shingles in candidates_uid_shingles:\n",
    "    candidate_shingles = set(candidate_shingles)\n",
    "    similarity = compute_jaccard_similarity(query_shingles, candidate_shingles)\n",
    "    # Check if the Jaccard similarity is above the threshold\n",
    "    if similarity > jaccard_threshold:\n",
    "        similar_news.append((candidate_uid, similarity))\n",
    "        similar_news_uid.append(candidate_uid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46ae14",
   "metadata": {},
   "source": [
    "#### Validation Of Results for sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d3466f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:====================================================> (182 + 7) / 189]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given article title: \n",
      "شیوه جدید فروش خودرو اعلام شد\n",
      "\n",
      "\n",
      "\n",
      "For the given article, 28 unique similar news founded:\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف شد\n",
      "\n",
      "UID: 3aaecce21996a6b83301e969f, Similarity: 0.45614035087719296\n",
      "\n",
      "\n",
      "Title: خبر فوری درباره سامانه یکپارچه خودرو / زمان عرضه مستقیم اعلام شد\n",
      "\n",
      "UID: 990cacaf05c55d5291602a1f4, Similarity: 1.0\n",
      "\n",
      "\n",
      "Title: شیوه جدید فروش خودرو اعلام شد + جزئیات\n",
      "\n",
      "UID: b27e0e25f39dae13486cfa9fe, Similarity: 0.7142857142857143\n",
      "\n",
      "\n",
      "Title: چرا سامانه یکپارچه خودرو حذف شد؟ | علت حذف دور سوم ثبت نام خودرو - اندیشه معاصر\n",
      "\n",
      "UID: 8381cae224fa2aa9b934cf381, Similarity: 0.5070422535211268\n",
      "\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف شد/ چگونه می‌شود از کارخانه خودرو خرید؟\n",
      "\n",
      "UID: 12f245b2790078eea668d2179, Similarity: 0.7592592592592593\n",
      "\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف شد- اخبار صنعت و تجارت - اخبار اقتصادی تسنیم | Tasnim\n",
      "\n",
      "UID: b9c4aad52e7eefbd4466102ec, Similarity: 0.7192982456140351\n",
      "\n",
      "\n",
      "Title: خبر مهم برای خریداران خودرو /اعلام شیوه جدید فروش خودرو\n",
      "\n",
      "UID: 9727a5eba5efe8ae25551e5fa, Similarity: 0.7068965517241379\n",
      "\n",
      "\n",
      "Title: خودرو را با این روش بخرید/ سامانه یکپارچه خودرو حذف شد + جزئیات\n",
      "\n",
      "UID: 7874021dc77ce123caaf3f033, Similarity: 0.49230769230769234\n",
      "\n",
      "\n",
      "Title: حذف سامانه یکپارچه فروش خودرو\n",
      "\n",
      "UID: e1d6f939be15d65230c5b2216, Similarity: 0.4444444444444444\n",
      "\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف شد/ فروش خودرو از ۸ مهر\n",
      "\n",
      "UID: f9221116b524ea14f18f0c1d5, Similarity: 0.6842105263157895\n",
      "\n",
      "\n",
      "Title: معاون وزیر صمت: سامانه یکپارچه خودرو حذف شد\n",
      "\n",
      "UID: e3870b775bb88e6b71f468c53, Similarity: 0.4915254237288136\n",
      "\n",
      "\n",
      "Title: فروش خودرو از طریق سامانه یکپارچه متوقف شد/ روش عرضه به صورت مستقیم و از کارخانه | مدار اقتصادی\n",
      "\n",
      "UID: 6ba70d8a80b2db6e7e0584da8, Similarity: 0.43548387096774194\n",
      "\n",
      "\n",
      "Title: فوری | سامانه یکپارچه فروش خودرو حذف شد\n",
      "\n",
      "UID: 1f318e048098dc77b367fc633, Similarity: 0.6896551724137931\n",
      "\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف شد | معاون صنایع حمل و نقل وزارت\n",
      "\n",
      "UID: adfc04dfae822cc49b2412aa9, Similarity: 0.37681159420289856\n",
      "\n",
      "\n",
      "Title: شیوه جدید فروش خودرو اعلام شد + جزئیات - اندیشه معاصر\n",
      "\n",
      "UID: b1326b393b02df3a12064ecb3, Similarity: 0.547945205479452\n",
      "\n",
      "\n",
      "Title: حذف شبانه سامانه یکپارچه خودرو!/ فروش مستقیم خودرو از کارخانه از ۸ مهر\n",
      "\n",
      "UID: 97adf9122e41ddb1f6f6d7b00, Similarity: 0.7592592592592593\n",
      "\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف ش\n",
      "\n",
      "UID: 0db68ee6e6f6695e0ecce36be, Similarity: 0.7068965517241379\n",
      "\n",
      "\n",
      "Title: طرح سامانه یکپارچه خودرو حذف شد \n",
      "\n",
      "UID: 6a0c5b54c7db11dbb8580b0ae, Similarity: 1.0\n",
      "\n",
      "\n",
      "Title: معاون وزیر صمت: سامانه یکپارچه خودرو حذف شد\n",
      "\n",
      "UID: 38f030b89882842729e3b165a, Similarity: 0.43243243243243246\n",
      "\n",
      "\n",
      "Title: فوری | سامانه یکپارچه خودرو حذف شد | اعلام زمان و جزئیات روش جدید فروش خودرو\n",
      "\n",
      "UID: c5022929cd7ec33408ee80c49, Similarity: 0.421875\n",
      "\n",
      "\n",
      "Title: روش فروش خودرو به قیمت کارخانه تغییر کرد + تاریخ عرضه جدید\n",
      "\n",
      "UID: 115b9fbe166a1641bd72df893, Similarity: 0.5555555555555556\n",
      "\n",
      "\n",
      "Title: فوری؛ سامانه یکپارچه خودرو حذف شد!  \n",
      "\n",
      "UID: 116e1951e6ec261e106a2ce0f, Similarity: 0.7592592592592593\n",
      "\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف شد\n",
      "\n",
      "UID: 3b92e8f292f26b2a9232b6cb2, Similarity: 0.4266666666666667\n",
      "\n",
      "\n",
      "Title: خبر مهم معاون وزیر صنعت برای بازار خودرو/ سامانه یکپارچه خودرو حذف شد؛ عرضه جدید خودرو چگونه خواهد بود؟\n",
      "\n",
      "UID: 8c239231f98f2fe1db1dfbd1b, Similarity: 0.6666666666666666\n",
      "\n",
      "\n",
      "Title: سامانه یکپارچه خودرو حذف شد\n",
      "\n",
      "UID: 2261fa398dbe5e07ba76ef9e2, Similarity: 0.4807692307692308\n",
      "\n",
      "\n",
      "Title: معاون وزیر صمت: سامانه یکپارچه خودرو حذف شد\n",
      "\n",
      "UID: 5d3c94171ec1f070ea33a4866, Similarity: 0.5\n",
      "\n",
      "\n",
      "Title: حذف سامانه یکپارچه/ فروش مستقیم توسط کارخانه‌ها\n",
      "\n",
      "UID: eb84398edeecb580b26600c12, Similarity: 0.4153846153846154\n",
      "\n",
      "\n",
      "Title: خبر شبانه معاون وزیر از حذف سامانه یکپارچه فروش خودرو/ عرضه جدید خودرو چگونه خواهد بود؟\n",
      "\n",
      "UID: d8cb06867f130e1e3100738bf, Similarity: 0.6229508196721312\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract the title of the query article\n",
    "query_title = list(set(news_rdd.filter(lambda x: json.loads(x)['uid'] in [query_uid]).map(lambda x: json.loads(x)['title']).collect()))\n",
    "\n",
    "# Extract the title and UID of similar news articles\n",
    "similar_news_list = list(set(news_rdd.filter(lambda x: json.loads(x)['uid'] in similar_news_uid).map(lambda x: (json.loads(x)['uid'], json.loads(x)['title'])).collect()))\n",
    "\n",
    "# Create dictionaries for efficient lookup\n",
    "dict1 = dict(similar_news)\n",
    "dict2 = dict(similar_news_list)\n",
    "\n",
    "# Join the dictionaries to get a list of tuples with common keys(uid)... final result is a list of tuples with uis, similarity and title of each similar news\n",
    "joined_list = [(key, dict1[key], dict2[key]) for key in set(dict1) & set(dict2)]\n",
    "\n",
    "# Display the query article title\n",
    "print('The given article title: ')\n",
    "print(query_title[0])\n",
    "print('\\n\\n')\n",
    "\n",
    "# Display information about similar news articles\n",
    "print(f'For the given article, {len(joined_list)} unique similar news founded:\\n')\n",
    "for uid, similarity, title in joined_list:\n",
    "    print(f\"Title: {title}\\n\")\n",
    "    print(f\"UID: {uid}, Similarity: {similarity}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
